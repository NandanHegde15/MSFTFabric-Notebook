{"cells":[{"cell_type":"code","source":["# install semantic-link-labs\n","%pip install semantic-link-labs"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e573efcf-27d4-4042-b922-1a9f7cff2326"},{"cell_type":"code","source":["Param_Lakehouse= \"\" #name or ID of the lakehouse where you want to save the #.pbip file\n","Param_LakehouseWorkspace =\"\" #name or ID of the workspace in which the lakehouse exists"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"5c917dc2-7969-4e34-869c-4d6c45c2fe37"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","import pandas as pd\n","import sempy_labs.report as rep\n","\n","# Get List of all workspaces\n","df_workspaces = fabric.list_workspaces().query('Type != \"AdminInsights\"')\n","\n","\n","\n","# Iterate over workspaces and fetch reports\n","all_reports = []\n","\n","for _, row in df_workspaces.iterrows():\n","    workspace_id = row['Id']\n","    workspace_name = row['Name']\n","    \n","    try:\n","        df_reports = fabric.list_reports(workspace_id)\n","        if not df_reports.empty:\n","            df_reports['ReportWorkspaceName'] = workspace_name\n","            df_reports['ReportWorkspaceId'] = workspace_id\n","            all_reports.append(df_reports)\n","    except Exception as e:\n","        print(f\"Error fetching reports for workspace '{workspace_name}': {e}\")\n","\n","# Combine all reports into one DataFrame\n","if all_reports:\n","    df_all_reports = pd.concat(all_reports, ignore_index=True)\n","\n","# If set to True, saves the report and underlying semantic model. If # set to False, saves just the report.\n","thick_report = True \n","\n","# If set to True, saves a .pbip live-connected to the workspace in \n","# the Power BI / Fabric service. If set to False, saves a .pbip with  # a local model, independent from the Power BI / Fabric service.\n","live_connect = True \n","\n","\n","for _, row in df_all_reports.iterrows():\n","    rep.save_report_as_pbip(report=row[\"Id\"], workspace=row[\"ReportWorkspaceId\"], thick_report=thick_report, live_connect=live_connect, lakehouse=Param_Lakehouse, lakehouse_workspace=Param_LakehouseWorkspace)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0fc898d5-75ac-41c2-b2ba-eb104cedeaa7"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}